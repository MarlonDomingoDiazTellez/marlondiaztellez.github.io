import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from imblearn.over_sampling import SMOTE

# Cargar datos
file_path = "cirrhosis.csv"
data = pd.read_csv(file_path)

# Preprocesamiento de datos
# Convertir variables categóricas a valores numéricos
label_encoder = LabelEncoder()
data['Sex'] = label_encoder.fit_transform(data['Sex'])
data['Ascites'] = label_encoder.fit_transform(data['Ascites'])
data['Hepatomegaly'] = label_encoder.fit_transform(data['Hepatomegaly'])
data['Spiders'] = label_encoder.fit_transform(data['Spiders'])
data['Edema'] = label_encoder.fit_transform(data['Edema'])
data['Drug'] = label_encoder.fit_transform(data['Drug'])

# Manejar valores faltantes
numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns
imputer = SimpleImputer(strategy='mean')
data[numeric_columns] = imputer.fit_transform(data[numeric_columns])

# Convertir la variable objetivo 'Stage' a clases discretas
def categorize_stage(stage):
    if stage <= 2:
        return 0  # Etapa temprana
    elif stage <= 4:
        return 1  # Etapa intermedia
    else:
        return 2  # Etapa avanzada

data['Stage'] = data['Stage'].apply(categorize_stage)

# Eliminar columnas irrelevantes
X = data.drop(columns=['ID', 'Status', 'Stage'])
Y = data['Stage']

# Normalizar características
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Balancear clases usando SMOTE
smote = SMOTE(random_state=42)
X_resampled, Y_resampled = smote.fit_resample(X_scaled, Y)

# Dividir datos en conjuntos de entrenamiento y prueba
X_train, X_test, Y_train, Y_test = train_test_split(X_resampled, Y_resampled, test_size=0.2, random_state=42)

# Configuración de hiperparámetros para Random Forest
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Búsqueda de hiperparámetros
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, Y_train)

# Mejor modelo encontrado
best_rf = grid_search.best_estimator_

# Hacer predicciones
Y_pred = best_rf.predict(X_test)

# Evaluar el modelo
accuracy = accuracy_score(Y_test, Y_pred)
report = classification_report(Y_test, Y_pred)

# Matriz de confusión
conf_matrix = confusion_matrix(Y_test, Y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", 
            xticklabels=['Temprana', 'Intermedia', 'Avanzada'], 
            yticklabels=['Temprana', 'Intermedia', 'Avanzada'])
plt.title("Matriz de Confusión")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.show()

# Mostrar resultados
grid_results = grid_search.cv_results_
plt.figure(figsize=(10, 6))
plt.plot(range(len(grid_results['mean_test_score'])), grid_results['mean_test_score'], marker='o')
plt.title("Precisión Promedio vs. Modelo")
plt.xlabel("Modelos probados")
plt.ylabel("Precisión Promedio")
plt.grid()
plt.show()

print(f"Mejores hiperparámetros encontrados: {grid_search.best_params_}")
print(f"Precisión del modelo: {accuracy:.2f}")
print("Reporte de clasificación:")
print(report)
